{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5aeb973-1009-495c-b8d1-b97c4bb54fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import backgammon\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "class BGBoard:\n",
    "    def __init__(self):\n",
    "        self.board = backgammon.Backgammon()\n",
    "        self.board.start()\n",
    "        self.turn = self.board.match.player\n",
    "        self.winner = -1\n",
    "        self.state_players = []\n",
    "        \n",
    "    def get_moves_and_positions(self):\n",
    "        possible_moves_positions = {}\n",
    "        for play in self.board.generate_plays():\n",
    "            pair = []\n",
    "            for move in play.moves:\n",
    "                pair.append((move.source,move.destination))\n",
    "            possible_moves_positions[tuple(pair)] = self.state(play.position)\n",
    "        return possible_moves_positions\n",
    "\n",
    "    def get_possible_moves(self, possible_moves_positions:dict):\n",
    "        possible_moves = possible_moves_positions.keys()\n",
    "        return list(possible_moves)\n",
    "    \n",
    "    def state(self,position):\n",
    "        # position = self.board.position\n",
    "        board_points = list(position.board_points)\n",
    "        p_bar = [position.player_bar]\n",
    "        p_off = [position.player_off]\n",
    "        op_bar = [position.opponent_bar]\n",
    "        op_off = [position.opponent_off]\n",
    "        die_1 = [self.board.match.dice[0]]\n",
    "        die_2 = [self.board.match.dice[1]]\n",
    "        current_player = [self.board.match.player.value]\n",
    "        state_list = board_points+p_bar+p_off+op_bar+op_off+die_1+die_2+current_player\n",
    "        state_array = np.array(state_list)/15\n",
    "        \n",
    "        return state_array\n",
    "\n",
    "    def make_move(self, move):\n",
    "        '''\n",
    "        move is tuple of tuples. e.g ((22,19),(23,21),...)\n",
    "        '''\n",
    "        self.state_players.append(int(self.board.match.turn.value))\n",
    "        if move == None:\n",
    "            self.board.skip()\n",
    "        else:\n",
    "            self.board.play(move)\n",
    "        \n",
    "        \n",
    "        if self.is_over():\n",
    "            return self.winner\n",
    "            \n",
    "        if self.board.match.dice == (0, 0):\n",
    "            self.board.roll()\n",
    "        \n",
    "    def is_over(self):\n",
    "        p0_score = self.board.match.player_0_score\n",
    "        p1_score = self.board.match.player_1_score\n",
    "        \n",
    "        if p0_score > p1_score:\n",
    "            self.winner = 0\n",
    "            return True\n",
    "        elif p0_score < p1_score:\n",
    "            self.winner = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # def get_reward(self):\n",
    "    #     player = self.board.match.player\n",
    "    #     if winner != -1:\n",
    "    #         if winner == player:\n",
    "    #             return torch.tensor(1,device=device, dtype=torch.float32)\n",
    "    #         elif winner != player:\n",
    "    #             return torch.tensor(-1,device=device, dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "143cf0c3-00a4-4c39-88d4-23dd866c5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(96, activation='relu', input_shape=(31,)), # 31 features for board state\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"tanh\")  # Output: Estimated value of the state\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00002)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "lambda_ = 0.7 # Eligibility trace decay\n",
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "        \n",
    "\n",
    "def td_lambda_update(self, states, rewards, model, alpha=0.00002, gamma=gamma, lambda_=lambda_):\n",
    "    \"\"\"\n",
    "    Perform TD(λ) updates on the model.\n",
    "\n",
    "    Args:\n",
    "        states: List of game states (numpy arrays).\n",
    "        rewards: List of rewards for each state.\n",
    "        model: The neural network model.\n",
    "        alpha: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        lambda_: Eligibility trace decay factor.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize eligibility traces with the same shapes as trainable variables\n",
    "    eligibility_traces = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "\n",
    "    for t in range(len(states) - 1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predict current and next state values\n",
    "            V_t = model(tf.convert_to_tensor([states[t]], dtype=tf.float32))  # Shape: (1, 1)\n",
    "            V_t_plus_1 = model(tf.convert_to_tensor([states[t + 1]], dtype=tf.float32))  # Shape: (1, 1)\n",
    "\n",
    "            # Compute TD error (delta_t)\n",
    "            delta_t = rewards[t] + gamma * tf.squeeze(V_t_plus_1) - tf.squeeze(V_t)\n",
    "\n",
    "        # Calculate gradients for the current state's value prediction\n",
    "        gradients = tape.gradient(V_t, model.trainable_variables)\n",
    "\n",
    "        # Update eligibility traces and model weights\n",
    "        for i in range(len(model.trainable_variables)):\n",
    "            # Ensure gradient and eligibility trace shapes match\n",
    "            if gradients[i] is not None:  # Some variables may not contribute to the gradient\n",
    "                eligibility_traces[i] = gamma * lambda_ * eligibility_traces[i] + gradients[i]\n",
    "                \n",
    "                # Ensure eligibility trace matches the variable's shape\n",
    "                eligibility_trace = tf.reshape(eligibility_traces[i], model.trainable_variables[i].shape)\n",
    "                \n",
    "                # Perform the TD(λ) weight update\n",
    "                model.trainable_variables[i].assign_add(alpha * delta_t * eligibility_trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc0c020-c2da-457b-9443-50e0ee39e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play_game(model,random_threshold=0.75):\n",
    "    states = []\n",
    "    b = BGBoard()\n",
    "    \n",
    "    while not b.is_over():\n",
    "        position = b.board.position\n",
    "        current_state = b.state(position)\n",
    "        # Append the current state (for the current player)\n",
    "        states.append(current_state)\n",
    "        \n",
    "        moves_positions = b.get_moves_and_positions()\n",
    "        possible_moves = b.get_possible_moves(moves_positions)\n",
    "        \n",
    "        \n",
    "        move_values = []\n",
    "        \n",
    "        if moves_positions != {}:\n",
    "            if random.uniform(0,1)<random_threshold:\n",
    "                best_move = random.choice(possible_moves)\n",
    "            \n",
    "            else:\n",
    "                    # Create a batch of next states for all possible moves\n",
    "                next_states = np.array([moves_positions[move] for move in possible_moves])\n",
    "                \n",
    "                # Predict values for all possible moves in a single batch\n",
    "                move_values = model.predict(next_states).flatten()  # Flatten to 1D array\n",
    "            \n",
    "                best_move = possible_moves[np.argmax(move_values)]\n",
    "        else:\n",
    "            best_move = None\n",
    "        b.make_move(best_move)\n",
    "        # print(b.board)\n",
    "    filter_array = np.array(b.state_players)\n",
    "    states = np.array(states)\n",
    "    states_winner = states[filter_array == b.winner]\n",
    "    states_loser = states[filter_array != b.winner]\n",
    "    final_rewards_winner = [0 for state_player in b.state_players if state_player == b.winner]\n",
    "    final_rewards_loser = [0 for state_player in b.state_players if state_player != b.winner]\n",
    "    final_rewards_winner[-1] = 1\n",
    "    final_rewards_loser[-1] = -1\n",
    "    # print(\"winner\")\n",
    "    # print(b.winner)\n",
    "    # print(final_rewards_winner)\n",
    "    # print(\"loser\")\n",
    "    # print(final_rewards_loser)\n",
    "    return states_winner, states_loser, final_rewards_winner, final_rewards_loser\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f379dec-28ee-4688-9218-b6edc56159f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 909/2500, threshold: 0.48179999999997997 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_epoch = 0\n",
    "epochs = 2500\n",
    "random_threshold = 0.3\n",
    "for epoch in range(last_epoch, epochs):  # Number of training games\n",
    "    print(f'progress: {epoch}/{epochs}, threshold: {random_threshold} ',end='\\r')\n",
    "    # Generate a game through self-play\n",
    "    states_winner,states_loser, rewards_winner, rewards_loser = play_game(model,random_threshold)\n",
    "    random_threshold += 0.000002\n",
    "    \n",
    "    # Perform TD(λ) updates\n",
    "    td_lambda_update(states_winner, rewards_winner, model)\n",
    "    td_lambda_update(states_loser,rewards_loser,model)\n",
    "\n",
    "    # Evaluate the model every 100 games\n",
    "    if epoch % 100 == 0:\n",
    "        model.save(f\"backgammon_RLmodel_64_02_{epoch}.h5\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
