{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f65e06-d12d-4295-b9fe-6d613fc62087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8980f7d3-dade-4e3c-8227-4d20817762c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_backgammon\n",
    "import time\n",
    "from itertools import count\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import numpy as np\n",
    "from gym_backgammon.envs.backgammon import WHITE, BLACK, COLORS, TOKEN\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "from itertools import count\n",
    "from gym_backgammon.envs.backgammon import WHITE, BLACK\n",
    "\n",
    "env = gym.make('gym_backgammon:backgammon-v0')\n",
    "\n",
    "# if GPU is available this code will state 1\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfefc978-d637-4c86-83ce-bf001e8b452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "exploration_rate = 0.9\n",
    "learning_rate = 0.001\n",
    "lambda_ = 0.8\n",
    "model = Sequential([\n",
    "                Dense(80, activation='relu', input_shape=(198,)), # 29 features for board state\n",
    "                Dense(1, activation=\"sigmoid\")  # Output: Estimated value of the state\n",
    "            ])\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer,\n",
    "      loss='mean_squared_error')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57421017-0ade-4618-9974-4519bf3b3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_update(observation, next_observation, reward, model, eligibility_traces, alpha=0.001, gamma=0.99, lambda_=0.8):\n",
    "    \"\"\"\n",
    "    Perform a TD(λ) update for a given state transition.\n",
    "\n",
    "    Args:\n",
    "        observation (array-like): Current observation/state.\n",
    "        next_observation (array-like): Next observation/state.\n",
    "        reward (float): Reward for the transition.\n",
    "        model (tf.keras.Model): The value function approximator.\n",
    "        eligibility_traces (list): List of eligibility traces for each parameter in the model.\n",
    "        alpha (float): Learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        lambda_ (float): Decay factor for eligibility traces.\n",
    "\n",
    "    Returns:\n",
    "        float: TD error for the transition.\n",
    "    \"\"\"\n",
    "    # Convert observations to tensors\n",
    "    observation_tensor = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "    next_observation_tensor = tf.convert_to_tensor([next_observation], dtype=tf.float32)\n",
    "\n",
    "    # Compute TD error and gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute V(s) and V(s') within the gradient context\n",
    "        value_s = tf.squeeze(model(observation_tensor))  # V(s)\n",
    "        value_s_next = tf.squeeze(model(next_observation_tensor))  # V(s')\n",
    "\n",
    "        # Compute TD error: δ = r + γ * V(s') - V(s)\n",
    "        td_error = reward + gamma * value_s_next - value_s\n",
    "\n",
    "    # Compute gradients of V(s) with respect to model parameters\n",
    "    gradients = tape.gradient(value_s, model.trainable_variables)\n",
    "\n",
    "    # Update eligibility traces and apply parameter updates\n",
    "    for i, grad in enumerate(gradients):\n",
    "        if grad is not None:\n",
    "            # Update eligibility trace\n",
    "            eligibility_traces[i] = lambda_ * eligibility_traces[i] + grad\n",
    "\n",
    "            # Update weights\n",
    "            model.trainable_variables[i].assign_add(alpha * td_error * eligibility_traces[i])\n",
    "\n",
    "    return td_error.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5f6dc1-495c-4b49-b93d-3bb234844073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "    def __init__(self, color, model):\n",
    "        self.color = color\n",
    "        self.model = model\n",
    "        self.name = 'TDAgent({})'.format(COLORS[color])\n",
    "\n",
    "    def roll_dice(self):\n",
    "        return (-randint(1, 6), -randint(1, 6)) if self.color == WHITE else (randint(1, 6), randint(1, 6))\n",
    "\n",
    "    def choose_best_action(self, actions, env,exploration_rate):\n",
    "        best_action = None\n",
    "        observations = []\n",
    "        if actions:\n",
    "            actions = list(actions)\n",
    "            \n",
    "            # print(exploration_rate)\n",
    "            if random.uniform(0,1)<exploration_rate:\n",
    "                best_action = random.choice(actions)\n",
    "            else:\n",
    "                tmp_counter = env.counter\n",
    "                env.counter = 0\n",
    "                state = env.game.save_state()\n",
    "                \n",
    "                # Iterate over all the legal moves and pick the best action\n",
    "                for i, action in enumerate(actions):\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    observations.append(observation)\n",
    "    \n",
    "                    # restore the board and other variables (undo the action)\n",
    "                    env.game.restore_state(state)\n",
    "                observations = np.array(observations)\n",
    "                values = self.model.predict(observations).flatten()\n",
    "                \n",
    "                # practical-issues-in-temporal-difference-learning, pag.3\n",
    "                # ... the network's output P_t is an estimate of White's probability of winning from board position x_t.\n",
    "                # ... the move which is selected at each time step is the move which maximizes P_t when White is to play and minimizes P_t when Black is to play.\n",
    "                best_action_index = int(np.argmax(values)) if self.color == WHITE else int(np.argmin(values))\n",
    "                best_action = actions[best_action_index]\n",
    "                env.counter = tmp_counter\n",
    "\n",
    "        return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e544ab43-b760-4c93-a4a2-55282e4a2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 0\n",
    "epochs = 30000\n",
    "total_losses = []\n",
    "td_alpha = 0.005\n",
    "wins = {WHITE: 0, BLACK: 0}\n",
    "\n",
    "\n",
    "agents = {WHITE: TDAgent(WHITE, model=model), BLACK: TDAgent(BLACK, model=model)}\n",
    "\n",
    "\n",
    "durations = []\n",
    "steps = 0\n",
    "start_training = time.time()\n",
    "\n",
    "# model = tf.keras.models.load_model(\"./gym_env_models/backgammon_RLmodel_gym_80_3_400.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d359389-71da-4e75-bdf3-1d34c7f8a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(last_epoch, epochs):\n",
    "    t = time.time()\n",
    "    if epoch % 1000 == 0 and td_alpha > 0.00005:\n",
    "        td_alpha *= 0.8\n",
    "      #   model.compile(optimizer=optimizer,\n",
    "      # loss='mean_squared_error')\n",
    "    if exploration_rate > 0.1:\n",
    "        exploration_rate -= 0.000001\n",
    "    \n",
    "    eligibility_traces = [tf.zeros_like(var) for var in model.trainable_variables]\n",
    "\n",
    "    agent_color, first_roll, observation = env.reset()\n",
    "    agent = agents[agent_color]\n",
    "\n",
    "    \n",
    "    agents = {WHITE: TDAgent(WHITE, model=model), BLACK: TDAgent(BLACK, model=model)}\n",
    "    losses = []\n",
    "    for i in count():\n",
    "        if first_roll:\n",
    "            roll = first_roll\n",
    "            first_roll = None\n",
    "        else:\n",
    "            roll = agent.roll_dice()\n",
    "    \n",
    "        # p = model.predict(np.array([observation]))\n",
    "        # print(p)\n",
    "        actions = env.get_valid_actions(roll)\n",
    "        action = agent.choose_best_action(actions, env, exploration_rate)\n",
    "        observation_next, reward, done, winner = env.step(action)\n",
    "        # p_next = model.predict(np.array([observation_next]))\n",
    "        \n",
    "        \n",
    "    \n",
    "        if done:\n",
    "            if winner is not None:\n",
    "                loss = td_lambda_update(observation, observation_next, reward, model, eligibility_traces, alpha=td_alpha)\n",
    "                \n",
    "                wins[agent.color] += 1\n",
    "    \n",
    "            tot = sum(wins.values())\n",
    "            tot = tot if tot > 0 else 1\n",
    "    \n",
    "            text = \"Game={:<6d} | Winner={} | after {:<4} plays || Wins: {}={:<6}({:<5.1f}%) | {}={:<6}({:<5.1f}%) | Duration={:<.3f} sec\".format(epoch + 1, winner, i,\n",
    "                agents[WHITE].name, wins[WHITE], (wins[WHITE] / tot) * 100,\n",
    "                agents[BLACK].name, wins[BLACK], (wins[BLACK] / tot) * 100, time.time() - t)\n",
    "            # print(text)\n",
    "            \n",
    "            durations.append(time.time() - t)\n",
    "            steps += i\n",
    "            # print(durations)\n",
    "            # print(steps)\n",
    "            break\n",
    "        else:\n",
    "            loss = td_lambda_update(observation, observation_next, reward, model, eligibility_traces, alpha=td_alpha)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        agent_color = env.get_opponent_agent()\n",
    "        agent = agents[agent_color]\n",
    "    \n",
    "        observation = observation_next\n",
    "\n",
    "    \n",
    "    last_epoch = epoch\n",
    "    if epoch % 100 == 0:\n",
    "        model.save(f\"./gym_env_models/backgammon_RLmodel_gym_80_5_{epoch}.h5\")   \n",
    "        \n",
    "    losses = [float(loss ** 2) for loss in losses]\n",
    "    loss_mean = sum(losses) / (len(losses) - 1)\n",
    "    total_losses.append(loss_mean)\n",
    "    # print(total_losses)\n",
    "    if len(total_losses) % 2 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(total_losses, label='TD Error (Loss)')\n",
    "        \n",
    "        if len(total_losses) >= 100:\n",
    "            rolling_mean = np.convolve(total_losses, np.ones(100)/100, mode='valid')\n",
    "            plt.plot(range(99, len(total_losses)), rolling_mean, label='Mean of last 100', color='orange', linestyle='--')\n",
    "        plt.xlabel(text)\n",
    "        plt.ylabel('TD Error (Loss)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f28f83-37de-40e6-9485-8f099f7e31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.8993759999999821 623\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate, exploration_rate,last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b43e0b-9a65-44ea-b639-dba1a99d2c5e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9cf208d-88e1-4770-8600-4d9a56e1ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game=200    | Winner=0 | after 52   plays || wins_eval: TDAgent(White)=115   (57.5 %) | TDAgent(Black)=85    (42.5 %) | Duration=1.673 sec\r"
     ]
    }
   ],
   "source": [
    "wins_eval = {WHITE: 0, BLACK: 0}\n",
    "steps_eval = 0\n",
    "games_to_evaluate = 200\n",
    "last_game = 0\n",
    "model_old = tf.keras.models.load_model(\"./gym_env_models/backgammon_RLmodel_gym_80_4_100.h5\")\n",
    "model_new = tf.keras.models.load_model(\"./gym_env_models/backgammon_RLmodel_gym_80_4_600.h5\")\n",
    "agents = {WHITE: TDAgent(WHITE, model=model_new), BLACK: TDAgent(BLACK, model=model_old)}\n",
    "\n",
    "for epoch in range(0, games_to_evaluate):\n",
    "    t = time.time()\n",
    "    \n",
    "    agent_color, first_roll, observation = env.reset()\n",
    "    agent = agents[agent_color]\n",
    "    \n",
    "    for i in count():\n",
    "        if first_roll:\n",
    "            roll = first_roll\n",
    "            first_roll = None\n",
    "        else:\n",
    "            roll = agent.roll_dice()\n",
    "    \n",
    "        \n",
    "        actions = env.get_valid_actions(roll)\n",
    "        action = agent.choose_best_action(actions, env)\n",
    "        observation_next, reward, done, winner = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            if winner is not None:\n",
    "                wins_eval[agent.color] += 1\n",
    "    \n",
    "            tot = sum(wins_eval.values())\n",
    "            tot = tot if tot > 0 else 1\n",
    "            \n",
    "            text = \"Game={:<6d} | Winner={} | after {:<4} plays || wins_eval: {}={:<6}({:<5.1f}%) | {}={:<6}({:<5.1f}%) | Duration={:<.3f} sec\".format(epoch + 1, winner, i,\n",
    "                agents[WHITE].name, wins_eval[WHITE], (wins_eval[WHITE] / tot) * 100,\n",
    "                agents[BLACK].name, wins_eval[BLACK], (wins_eval[BLACK] / tot) * 100, time.time() - t)\n",
    "            print(text, end='\\r')\n",
    "            \n",
    "            # durations.append(time.time() - t)\n",
    "            steps_eval += i\n",
    "            # print(durations)\n",
    "            # print(steps)\n",
    "            break\n",
    "        \n",
    "        agent_color = env.get_opponent_agent()\n",
    "        agent = agents[agent_color]\n",
    "        last_game = epoch\n",
    "        observation = observation_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "801a52ab-8195-448b-873c-f93e4085df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_rate = 0.9\n",
    "for i in range(1000):\n",
    "    exploration_rate -= 0.00003\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2f5c5f2-c16f-45b6-92a0-61f73de98bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8700000000000255\n"
     ]
    }
   ],
   "source": [
    "print(exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3f7a3-e806-478b-a39f-bd6d31d94b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
